seed: 42
device: auto  # auto-detect: cuda > mps > cpu; or set explicitly: cuda, mps, cpu
dtype: bf16  # bf16, fp32 (bf16 supported on A100, H100, Apple Silicon M1+)

tokenizer:
  name_or_path: "Qwen/Qwen2.5-7B"
  normalize: none
  lowercase: false

sequence:
  max_len: 512
  pad_to_max: true

mlm:
  mask_prob: 0.15
  replace_probs:
    mask: 0.8
    random: 0.1
    keep: 0.1

model:
  d_model: 3584  # Qwen 7B hidden size
  n_heads: 16
  n_layers: 2
  d_ff: 14336  # 4 * d_model
  dropout: 0.1
  rope_theta: 10000.0

optim:
  name: adamw
  lr_peak: 3e-4
  betas: [0.9, 0.95]
  weight_decay: 0.01
  grad_clip: 1.0

schedule:
  type: cosine
  warmup_ratio: 0.05
  epochs: 12

train:
  global_batch_tokens: 131072  # total tokens per update (across accumulation steps)
  micro_batch_size: 8  # samples per forward pass
  log_interval: 50
  ckpt_every_steps: 500
  save_best_by: "val_mlm_loss"
  use_amp: true  # automatic mixed precision
  gradient_checkpointing: true

export:
  pool: "cls"  # "cls" or "mean"
  out_name: "contextualizer_v1_qwen7b_3584"
